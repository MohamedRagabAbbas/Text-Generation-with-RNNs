{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2730445,"sourceType":"datasetVersion","datasetId":1167113},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":7112387,"sourceType":"datasetVersion","datasetId":4101209},{"sourceId":8236291,"sourceType":"datasetVersion","datasetId":4885189}],"dockerImageVersionId":30716,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"fgxYk7rEsHOp","execution":{"iopub.status.busy":"2024-06-02T21:50:04.060807Z","iopub.execute_input":"2024-06-02T21:50:04.061157Z","iopub.status.idle":"2024-06-02T21:50:32.831440Z","shell.execute_reply.started":"2024-06-02T21:50:04.061124Z","shell.execute_reply":"2024-06-02T21:50:32.830492Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"D0602 21:50:07.697585670    2751 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD0602 21:50:07.697612431    2751 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD0602 21:50:07.697616093    2751 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD0602 21:50:07.697618575    2751 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD0602 21:50:07.697621015    2751 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD0602 21:50:07.697623338    2751 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD0602 21:50:07.697625755    2751 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD0602 21:50:07.697628007    2751 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD0602 21:50:07.697630227    2751 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD0602 21:50:07.697632436    2751 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD0602 21:50:07.697634670    2751 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD0602 21:50:07.697636928    2751 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD0602 21:50:07.697639161    2751 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD0602 21:50:07.697641396    2751 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD0602 21:50:07.697643610    2751 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD0602 21:50:07.697645846    2751 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD0602 21:50:07.697648236    2751 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD0602 21:50:07.697650520    2751 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD0602 21:50:07.697652780    2751 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD0602 21:50:07.697655064    2751 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD0602 21:50:07.697658978    2751 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD0602 21:50:07.697661325    2751 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD0602 21:50:07.697663653    2751 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD0602 21:50:07.697665908    2751 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD0602 21:50:07.697668092    2751 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD0602 21:50:07.697670323    2751 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD0602 21:50:07.697672642    2751 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD0602 21:50:07.697674948    2751 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD0602 21:50:07.697677323    2751 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD0602 21:50:07.697680590    2751 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD0602 21:50:07.697682938    2751 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD0602 21:50:07.697685223    2751 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD0602 21:50:07.697687590    2751 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD0602 21:50:07.697689921    2751 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD0602 21:50:07.697692168    2751 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD0602 21:50:07.697694404    2751 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD0602 21:50:07.697696594    2751 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD0602 21:50:07.697698820    2751 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD0602 21:50:07.697701130    2751 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD0602 21:50:07.697703423    2751 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD0602 21:50:07.697705633    2751 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD0602 21:50:07.697707866    2751 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD0602 21:50:07.697710126    2751 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD0602 21:50:07.697712418    2751 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD0602 21:50:07.697714700    2751 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI0602 21:50:07.702722241    2751 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD0602 21:50:07.702759905    2751 ev_posix.cc:113]                      Using polling engine: epoll1\nD0602 21:50:07.727141965    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0602 21:50:07.727156642    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0602 21:50:07.727963954    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0602 21:50:07.727969483    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0602 21:50:07.727972799    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0602 21:50:07.727975758    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD0602 21:50:07.730956628    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0602 21:50:07.731001100    2751 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD0602 21:50:07.732274042    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0602 21:50:07.733425892    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0602 21:50:07.734189167    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0602 21:50:07.734196312    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0602 21:50:07.734201840    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0602 21:50:07.734205723    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0602 21:50:07.734209646    2751 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0602 21:50:07.734218644    2751 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD0602 21:50:07.734255768    2751 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI0602 21:50:07.736832450    2751 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI0602 21:50:07.744013239    2751 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/daigt-proper-train-dataset/train_drcat_03.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T21:50:32.832861Z","iopub.execute_input":"2024-06-02T21:50:32.833316Z","iopub.status.idle":"2024-06-02T21:50:34.970539Z","shell.execute_reply.started":"2024-06-02T21:50:32.833287Z","shell.execute_reply":"2024-06-02T21:50:34.969699Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def prepare_text_data(text, max_sequence_len, max_vocab_size=10000):\n    # Initialize the tokenizer with a limited vocabulary size\n    tokenizer = Tokenizer(num_words=max_vocab_size)\n    tokenizer.fit_on_texts([text])\n    total_words = min(max_vocab_size, len(tokenizer.word_index) + 1)\n\n    input_sequences = []\n    for line in text.split('\\n'):\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n    label = to_categorical(label, num_classes=total_words)\n\n    return predictors, label, tokenizer, total_words","metadata":{"id":"LEpOg28ysmj5","execution":{"iopub.status.busy":"2024-06-02T21:50:34.971548Z","iopub.execute_input":"2024-06-02T21:50:34.971857Z","iopub.status.idle":"2024-06-02T21:50:34.978266Z","shell.execute_reply.started":"2024-06-02T21:50:34.971830Z","shell.execute_reply":"2024-06-02T21:50:34.977494Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T21:50:34.979872Z","iopub.execute_input":"2024-06-02T21:50:34.980146Z","iopub.status.idle":"2024-06-02T21:50:35.009521Z","shell.execute_reply.started":"2024-06-02T21:50:34.980111Z","shell.execute_reply":"2024-06-02T21:50:35.008876Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       essay_id                                               text  label  \\\n0  6BB4BEB51A80  People can learn without making mistakes. Peop...      0   \n1  30A8FB981469  PHONES AND DRIVING\\n\\nIn this world in which w...      0   \n2  B403A4E28BCE  Okay, here's my essay:\\n\\nMaking Your Own Deci...      1   \n3  B8F0ECC9DC86   Dear : Principal\\n\\nI believe that allowing s...      1   \n4  159424F57C24  Well for one if you seek more then one person ...      0   \n\n                 source                                             prompt  \\\n0         original_moth  Task: \\n\\nWrite an essay examining the ways in...   \n1       persuade_corpus                                                NaN   \n2           llama2_chat  Task: Write an essay exploring why teenagers s...   \n3  mistral7binstruct_v2  \\nTask: Should students be encouraged to parti...   \n4       persuade_corpus                                                NaN   \n\n   fold  \n0     1  \n1     2  \n2     8  \n3     8  \n4     8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>source</th>\n      <th>prompt</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6BB4BEB51A80</td>\n      <td>People can learn without making mistakes. Peop...</td>\n      <td>0</td>\n      <td>original_moth</td>\n      <td>Task: \\n\\nWrite an essay examining the ways in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30A8FB981469</td>\n      <td>PHONES AND DRIVING\\n\\nIn this world in which w...</td>\n      <td>0</td>\n      <td>persuade_corpus</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B403A4E28BCE</td>\n      <td>Okay, here's my essay:\\n\\nMaking Your Own Deci...</td>\n      <td>1</td>\n      <td>llama2_chat</td>\n      <td>Task: Write an essay exploring why teenagers s...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B8F0ECC9DC86</td>\n      <td>Dear : Principal\\n\\nI believe that allowing s...</td>\n      <td>1</td>\n      <td>mistral7binstruct_v2</td>\n      <td>\\nTask: Should students be encouraged to parti...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>159424F57C24</td>\n      <td>Well for one if you seek more then one person ...</td>\n      <td>0</td>\n      <td>persuade_corpus</td>\n      <td>NaN</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"prepare_text_data(train.iloc[1].text,15)","metadata":{"id":"LdR2NBTgsmg9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"744d5087-88e7-4041-f045-6d8a3d2d9cec","execution":{"iopub.status.busy":"2024-06-02T21:50:35.010370Z","iopub.execute_input":"2024-06-02T21:50:35.010604Z","iopub.status.idle":"2024-06-02T21:50:35.020178Z","shell.execute_reply.started":"2024-06-02T21:50:35.010581Z","shell.execute_reply":"2024-06-02T21:50:35.019473Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(array([[  0,   0,   0, ...,   0,   0,  17],\n        [  0,   0,   0, ...,   0,  17,   3],\n        [  0,   0,   0, ...,   0,   0,  10],\n        ...,\n        [  3,   9,  15, ...,  32,  33, 191],\n        [  9,  15,   5, ...,  33, 191,  69],\n        [ 15,   5,   8, ..., 191,  69,  12]], dtype=int32),\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 1.]]),\n <keras.src.legacy.preprocessing.text.Tokenizer at 0x7c10aa6e3b20>,\n 193)"},"metadata":{}}]},{"cell_type":"code","source":"# Combine text data for tokenization\nif train is not None:\n    #train_data, _ = train_test_split(train, test_size=0.5, random_state=5)\n    combined_text = ' '.join(train['text'].tolist())\n\n    # Tokenize and prepare sequences\n    max_sequence_len = 40\n    max_vocab_size = 1500  # Limit vocabulary size\n    predictors, label, tokenizer, total_words = prepare_text_data(combined_text, max_sequence_len,max_vocab_size)","metadata":{"id":"Mk9-BUBesmbG","execution":{"iopub.status.busy":"2024-06-02T21:50:48.998340Z","iopub.execute_input":"2024-06-02T21:50:48.999206Z","iopub.status.idle":"2024-06-02T21:53:32.453510Z","shell.execute_reply.started":"2024-06-02T21:50:48.999165Z","shell.execute_reply":"2024-06-02T21:53:32.452107Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(predictors[0])\nprint(label[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:20:30.255484Z","iopub.execute_input":"2024-06-03T01:20:30.255848Z","iopub.status.idle":"2024-06-03T01:20:30.261684Z","shell.execute_reply.started":"2024-06-03T01:20:30.255811Z","shell.execute_reply":"2024-06-03T01:20:30.260795Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25]\n[0. 0. 0. ... 0. 0. 0.]\n","output_type":"stream"}]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.TPUStrategy(tpu)\n\ndef build_model(max_sequence_len, total_words):\n    with tpu_strategy.scope():\n        model = tf.keras.Sequential([\n            tf.keras.layers.Embedding(total_words, 32, input_length=max_sequence_len-1),\n            tf.keras.layers.LSTM(32),\n            tf.keras.layers.Dense(total_words, activation='softmax')\n        ])\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n        return model","metadata":{"execution":{"iopub.status.busy":"2024-06-02T21:53:32.455397Z","iopub.execute_input":"2024-06-02T21:53:32.455792Z","iopub.status.idle":"2024-06-02T21:53:39.781554Z","shell.execute_reply.started":"2024-06-02T21:53:32.455764Z","shell.execute_reply":"2024-06-02T21:53:39.780667Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717365215.474427    2751 service.cc:145] XLA service 0x5579417a1980 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1717365215.474530    2751 service.cc:153]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1717365215.474535    2751 service.cc:153]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1717365215.474538    2751 service.cc:153]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1717365215.474542    2751 service.cc:153]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1717365215.474544    2751 service.cc:153]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1717365215.474547    2751 service.cc:153]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1717365215.474550    2751 service.cc:153]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1717365215.474552    2751 service.cc:153]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"}]},{"cell_type":"code","source":" # Build and compile the model\nmodel = build_model(max_sequence_len, total_words)\n\n# Train the model\nmodel.fit(predictors, label, epochs=20,batch_size=256, verbose=1)\n#model.fit(predictors, label, epochs=5, verbose=1, callbacks=[])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T21:53:39.782686Z","iopub.execute_input":"2024-06-02T21:53:39.782993Z","iopub.status.idle":"2024-06-03T01:20:30.253165Z","shell.execute_reply.started":"2024-06-02T21:53:39.782962Z","shell.execute_reply":"2024-06-03T01:20:30.252008Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\nI0000 00:00:1717365220.084697    2751 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2024-06-02 21:59:13.213768: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.\nI0000 00:00:1717365554.291269    3559 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(1dfcdfefcdf31df8:0:0), session_name()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   15/57482\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:26\u001b[0m 12ms/step - accuracy: 0.0245 - loss: 7.3105  ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1717365555.783506    3559 tpu_compile_op_common.cc:245] Compilation of 1dfcdfefcdf31df8:0:0 with session name  took 1.492175926s and succeeded\nI0000 00:00:1717365555.790423    3559 tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(1dfcdfefcdf31df8:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_iterator_10443328887483456123\", property.function_library_fingerprint = 7656539742441471266, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,39,;32,1500,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1717365555.790472    3559 tpu_compilation_cache_interface.cc:541] After adding entry for key 1dfcdfefcdf31df8:0:0 with session_name  cache is 1 entries (7532960 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m57479/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1179 - loss: 5.1781","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1717366156.772570    3507 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(68e7849b6daf4c89:0:0), session_name()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 10ms/step - accuracy: 0.1179 - loss: 5.1781\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1717366157.783694    3507 tpu_compile_op_common.cc:245] Compilation of 68e7849b6daf4c89:0:0 with session name  took 1.011065568s and succeeded\nI0000 00:00:1717366157.790371    3507 tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(68e7849b6daf4c89:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_iterator_10443328887483456123\", property.function_library_fingerprint = 7656539742441471266, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"12,39,;12,1500,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1717366157.790407    3507 tpu_compilation_cache_interface.cc:541] After adding entry for key 68e7849b6daf4c89:0:0 with session_name  cache is 2 entries (14618821 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 10ms/step - accuracy: 0.1832 - loss: 4.4918\nEpoch 3/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 10ms/step - accuracy: 0.1958 - loss: 4.3614\nEpoch 4/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 10ms/step - accuracy: 0.2027 - loss: 4.2942\nEpoch 5/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 10ms/step - accuracy: 0.2069 - loss: 4.2535\nEpoch 6/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 10ms/step - accuracy: 0.2099 - loss: 4.2311\nEpoch 7/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 10ms/step - accuracy: 0.2118 - loss: 4.2093\nEpoch 8/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 10ms/step - accuracy: 0.2134 - loss: 4.1918\nEpoch 9/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 10ms/step - accuracy: 0.2133 - loss: 4.1813\nEpoch 10/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 11ms/step - accuracy: 0.2153 - loss: 4.1717\nEpoch 11/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 11ms/step - accuracy: 0.2154 - loss: 4.1647\nEpoch 12/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 10ms/step - accuracy: 0.2171 - loss: 4.1546\nEpoch 13/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 10ms/step - accuracy: 0.2167 - loss: 4.1501\nEpoch 14/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 10ms/step - accuracy: 0.2179 - loss: 4.1463\nEpoch 15/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 10ms/step - accuracy: 0.2170 - loss: 4.1427\nEpoch 16/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 10ms/step - accuracy: 0.2189 - loss: 4.1388\nEpoch 17/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 10ms/step - accuracy: 0.2185 - loss: 4.1342\nEpoch 18/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 10ms/step - accuracy: 0.2193 - loss: 4.1284\nEpoch 19/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 10ms/step - accuracy: 0.2187 - loss: 4.1255\nEpoch 20/20\n\u001b[1m57482/57482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 10ms/step - accuracy: 0.2189 - loss: 4.1254\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7c10753b7d90>"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nprint(tf.__version__)\nprint(keras.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Save the model to a file\nmodel.save('text_generation_model01.h5')\n\n# Create a download link\nFileLink('text_generation_model01.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:44:21.055356Z","iopub.execute_input":"2024-06-03T01:44:21.055631Z","iopub.status.idle":"2024-06-03T01:44:21.100779Z","shell.execute_reply.started":"2024-06-03T01:44:21.055603Z","shell.execute_reply":"2024-06-03T01:44:21.099498Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileLink\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save the model to a file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_generation_model01.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a download link\u001b[39;00m\n\u001b[1;32m      7\u001b[0m FileLink(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_generation_model01.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/distribute/packed_distributed_variable.py:91\u001b[0m, in \u001b[0;36mPackedDistributedVariable.get_var_on_device\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m==\u001b[39m device:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_variables[i]\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m device)\n","\u001b[0;31mValueError\u001b[0m: Device /job:localhost/replica:0/task:0/device:CPU:0 is not found"],"ename":"ValueError","evalue":"Device /job:localhost/replica:0/task:0/device:CPU:0 is not found","output_type":"error"}]},{"cell_type":"code","source":"# Function to generate text\ndef generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict(token_list, verbose=0)\n        predicted_word_index = np.argmax(predicted, axis=-1)\n        output_word = tokenizer.index_word[predicted_word_index[0]]\n        seed_text += \" \" + output_word\n    return seed_text","metadata":{"id":"kQjtty2ZsmXS","execution":{"iopub.status.busy":"2024-06-03T01:37:39.226422Z","iopub.execute_input":"2024-06-03T01:37:39.226974Z","iopub.status.idle":"2024-06-03T01:37:49.197456Z","shell.execute_reply.started":"2024-06-03T01:37:39.226941Z","shell.execute_reply":"2024-06-03T01:37:49.196312Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Generate and print sample text\nprint(generate_text(\"Once upon a time making\", 10, model, max_sequence_len))","metadata":{"id":"X4DIM0ZSsmQt","execution":{"iopub.status.busy":"2024-06-03T01:38:54.963014Z","iopub.execute_input":"2024-06-03T01:38:54.963725Z","iopub.status.idle":"2024-06-03T01:39:01.123326Z","shell.execute_reply.started":"2024-06-03T01:38:54.963690Z","shell.execute_reply":"2024-06-03T01:39:01.122282Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Once upon a time making a difference in the world and the people can be\n","output_type":"stream"}]},{"cell_type":"code","source":"print(generate_text(\"The cat is \", 10, model, max_sequence_len))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:46:54.025807Z","iopub.execute_input":"2024-06-03T01:46:54.026665Z","iopub.status.idle":"2024-06-03T01:47:00.266007Z","shell.execute_reply.started":"2024-06-03T01:46:54.026630Z","shell.execute_reply":"2024-06-03T01:47:00.264913Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"The cat is  a good idea to be able to do it and\n","output_type":"stream"}]},{"cell_type":"code","source":"print(generate_text(\"I'am  \", 10, model, max_sequence_len))","metadata":{"id":"hEfmHW90smEx","trusted":true},"execution_count":null,"outputs":[]}]}